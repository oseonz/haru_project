"""
ì§€ì‹ë² ì´ìŠ¤ ê´€ë¦¬ ëª¨ë“ˆ
ë°ì´í„° ë¡œë”©, ì²˜ë¦¬, ë²¡í„°í™” ë‹´ë‹¹
"""

import pandas as pd
from pathlib import Path
from typing import List
from langchain_openai import OpenAIEmbeddings  
from langchain_community.vectorstores import FAISS
from langchain.text_splitter import CharacterTextSplitter


class KnowledgeBaseManager:
    """ì§€ì‹ë² ì´ìŠ¤ ê´€ë¦¬ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.knowledge_base = None
    
    def process_large_food_csv(self, file_path: Path, chunk_size: int = 1000) -> List[str]:
        """ëŒ€ìš©ëŸ‰ ìŒì‹ CSV íŒŒì¼ì„ ì²­í¬ ë‹¨ìœ„ë¡œ ì²˜ë¦¬"""
        chunks = []
        
        try:
            print(f"ëŒ€ìš©ëŸ‰ íŒŒì¼ ì²˜ë¦¬ ì‹œì‘: {file_path.name}")
            
            # ğŸ”„ ë‹¤ì–‘í•œ ì¸ì½”ë”© ì‹œë„ (í•œê¸€ CSV íŒŒì¼ ëŒ€ì‘)
            encodings_to_try = ['utf-8', 'cp949', 'euc-kr', 'latin-1']
            chunk_iter = None
            
            for encoding in encodings_to_try:
                try:
                    chunk_iter = pd.read_csv(file_path, chunksize=chunk_size, encoding=encoding)
                    print(f"  ì„±ê³µí•œ ì¸ì½”ë”©: {encoding}")
                    break
                except UnicodeDecodeError:
                    print(f"  ì‹¤íŒ¨í•œ ì¸ì½”ë”©: {encoding}")
                    continue
            
            if chunk_iter is None:
                print(f"  ëª¨ë“  ì¸ì½”ë”© ì‹¤íŒ¨: {file_path.name}")
                return []
            
            for i, chunk_df in enumerate(chunk_iter):
                # ê° ì²­í¬ë¥¼ ì˜ë¯¸ìˆëŠ” í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
                processed_texts = []
                
                for _, row in chunk_df.iterrows():
                    try:
                        # í–‰ì„ ìì—°ì–´ ë¬¸ì¥ìœ¼ë¡œ ë³€í™˜
                        row_text = self.convert_nutrition_row_to_text(row, chunk_df.columns)
                        if row_text:
                            processed_texts.append(row_text)
                    except Exception as e:
                        continue
                
                # ì²­í¬ë³„ë¡œ í…ìŠ¤íŠ¸ ê²°í•©
                if processed_texts:
                    chunk_text = f"=== {file_path.name} ì²­í¬ {i+1} ===\n" + "\n".join(processed_texts)
                    chunks.append(chunk_text)
                
                # ì§„í–‰ìƒí™© ì¶œë ¥
                if i % 5 == 0:
                    print(f"  ì²˜ë¦¬ë¨: {i * chunk_size}í–‰")
                    
        except Exception as e:
            print(f"ëŒ€ìš©ëŸ‰ CSV ì²˜ë¦¬ ì˜¤ë¥˜ ({file_path.name}): {e}")
            return []
        
        print(f"ì™„ë£Œ: {file_path.name} - {len(chunks)}ê°œ ì²­í¬ ìƒì„±")
        return chunks

    def convert_nutrition_row_to_text(self, row, columns) -> str:
        """ì˜ì–‘ì„±ë¶„ ë°ì´í„° í–‰ì„ ìì—°ì–´ ë¬¸ì¥ìœ¼ë¡œ ë³€í™˜"""
        
        # ì»¬ëŸ¼ëª…ì—ì„œ ì£¼ìš” ì •ë³´ ì¶”ì¶œ
        food_name = None
        nutrition_info = {}
        
        for col in columns:
            value = row[col]
            if pd.isna(value) or value == '':
                continue
                
            col_lower = str(col).lower()
            
            # ìŒì‹ëª… ì¶”ì¶œ (ì²« ë²ˆì§¸ ì»¬ëŸ¼ì´ë‚˜ 'ëª…' í¬í•¨ ì»¬ëŸ¼)
            if food_name is None and ('ëª…' in col or col == columns[0]):
                food_name = str(value)
            
            # ì˜ì–‘ì„±ë¶„ ì¶”ì¶œ
            elif any(keyword in col_lower for keyword in ['ì¹¼ë¡œë¦¬', 'ì—´ëŸ‰', 'kcal']):
                nutrition_info['ì¹¼ë¡œë¦¬'] = value
            elif any(keyword in col_lower for keyword in ['ë‹¨ë°±ì§ˆ', 'protein']):
                nutrition_info['ë‹¨ë°±ì§ˆ'] = value
            elif any(keyword in col_lower for keyword in ['íƒ„ìˆ˜í™”ë¬¼', 'carb']):
                nutrition_info['íƒ„ìˆ˜í™”ë¬¼'] = value
            elif any(keyword in col_lower for keyword in ['ì§€ë°©', 'fat']):
                nutrition_info['ì§€ë°©'] = value
            elif any(keyword in col_lower for keyword in ['ë‚˜íŠ¸ë¥¨', 'sodium']):
                nutrition_info['ë‚˜íŠ¸ë¥¨'] = value
            elif any(keyword in col_lower for keyword in ['ë‹¹ë¥˜', 'ë‹¹ë¶„', 'sugar']):
                nutrition_info['ë‹¹ë¥˜'] = value
            elif any(keyword in col_lower for keyword in ['í¬í™”ì§€ë°©']):
                nutrition_info['í¬í™”ì§€ë°©'] = value
        
        # ìì—°ì–´ ë¬¸ì¥ ìƒì„±
        if not food_name:
            return ""
        
        text_parts = [f"{food_name}"]
        
        if nutrition_info:
            nutrition_parts = []
            for key, value in nutrition_info.items():
                if value and str(value) != 'nan':
                    nutrition_parts.append(f"{key} {value}")
            
            if nutrition_parts:
                text_parts.append("ì˜ì–‘ì„±ë¶„: " + ", ".join(nutrition_parts))
        
        return " - ".join(text_parts) + "."

    def init_knowledge_base(self, file_paths: List[str]):
        """ê°œì„ ëœ ì§€ì‹ë² ì´ìŠ¤ ì´ˆê¸°í™”"""
        all_texts = []
        
        for file_path in file_paths:
            file_path = Path(file_path)
            suffix = file_path.suffix.lower()
            file_size = file_path.stat().st_size / (1024 * 1024)  # MB
            
            print(f"íŒŒì¼ ì²˜ë¦¬: {file_path.name} ({file_size:.1f}MB)")
            
            # ğŸš« ëŒ€ìš©ëŸ‰ íŒŒì¼ ìŠ¤í‚µ (ì„ë² ë”© API ì œí•œ ëŒ€ì‘)
            if file_size > 10:  # 10MB ì´ìƒ íŒŒì¼ ì œì™¸
                print(f"  âš ï¸ ëŒ€ìš©ëŸ‰ íŒŒì¼ ìŠ¤í‚µ: {file_path.name} (ì„ë² ë”© ì²˜ë¦¬ ì œí•œ)")
                continue
            
            if suffix == ".txt":
                # ê¸°ì¡´ í…ìŠ¤íŠ¸ íŒŒì¼ ì²˜ë¦¬
                try:
                    with open(file_path, "r", encoding="utf-8") as f:
                        content = f.read()
                    all_texts.append(content)
                except Exception as e:
                    print(f"í…ìŠ¤íŠ¸ íŒŒì¼ ì˜¤ë¥˜: {e}")
                    
            elif suffix == ".csv":
                # ğŸ”„ ì†Œìš©ëŸ‰ íŒŒì¼ë§Œ ì²˜ë¦¬ (ì¸ì½”ë”© ê°œì„ )
                try:
                    # ğŸ”„ ë‹¤ì–‘í•œ ì¸ì½”ë”© ì‹œë„
                    encodings_to_try = ['utf-8', 'cp949', 'euc-kr', 'latin-1']
                    df = None
                    
                    for encoding in encodings_to_try:
                        try:
                            df = pd.read_csv(file_path, encoding=encoding)
                            print(f"  {file_path.name} ì„±ê³µí•œ ì¸ì½”ë”©: {encoding}")
                            break
                        except UnicodeDecodeError:
                            continue
                    
                    if df is not None:
                        # ğŸ”„ ë°ì´í„° ìƒ˜í”Œë§ìœ¼ë¡œ í¬ê¸° ì¤„ì´ê¸°
                        if len(df) > 1000:
                            df = df.sample(n=1000, random_state=42)
                            print(f"  âœ‚ï¸ ë°ì´í„° ìƒ˜í”Œë§: {len(df)}í–‰ìœ¼ë¡œ ì¶•ì†Œ")
                        
                        all_texts.append(df.to_string(index=False))
                    else:
                        print(f"  {file_path.name} ëª¨ë“  ì¸ì½”ë”© ì‹¤íŒ¨")
                        
                except Exception as e:
                    print(f"CSV íŒŒì¼ ì˜¤ë¥˜: {e}")
        
        # CharacterTextSplitterë¡œ ìµœì¢… ì²­í¬ ë¶„í•  (ğŸ”„ ì²­í¬ í¬ê¸° ì¦ê°€)
        text_splitter = CharacterTextSplitter(
            separator="\n",
            chunk_size=3000,  # ğŸ”„ ì¦ê°€: 1500 -> 3000
            chunk_overlap=300   # ğŸ”„ ì¦ê°€: 200 -> 300
        )
        
        final_chunks = []
        for text in all_texts:
            if text:
                text_chunks = text_splitter.split_text(text)
                final_chunks.extend(text_chunks)
        
        print(f"ìµœì¢… ì²­í¬ ìˆ˜: {len(final_chunks)}")
        
        # ğŸ”„ ì²­í¬ ìˆ˜ ì œí•œ (OpenAI API ì œí•œ ëŒ€ì‘)
        if len(final_chunks) > 2000:
            final_chunks = final_chunks[:2000]
            print(f"  âœ‚ï¸ ì²­í¬ ìˆ˜ ì œí•œ: {len(final_chunks)}ê°œë¡œ ì¶•ì†Œ")
        
        # OpenAI Embeddingsë¡œ ë²¡í„°í™” (ğŸ”„ ë°°ì¹˜ í¬ê¸° ì¡°ì •)
        embeddings = OpenAIEmbeddings(
            model="text-embedding-ada-002",
            chunk_size=100  # ğŸ”„ ê¸°ë³¸ê°’(1000)ì—ì„œ 100ìœ¼ë¡œ ì¶•ì†Œ
        )
        
        self.knowledge_base = FAISS.from_texts(final_chunks, embeddings)
        return self.knowledge_base

    def get_knowledge_base(self):
        """í˜„ì¬ ì§€ì‹ë² ì´ìŠ¤ ë°˜í™˜"""
        return self.knowledge_base
    
    def search(self, query: str, k: int = 4):
        """ì§€ì‹ë² ì´ìŠ¤ì—ì„œ ìœ ì‚¬í•œ ë¬¸ì„œ ê²€ìƒ‰"""
        if self.knowledge_base is None:
            return []
        return self.knowledge_base.similarity_search(query, k=k)